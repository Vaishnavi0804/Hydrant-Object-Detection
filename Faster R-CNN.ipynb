{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34eba1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5ab5a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from lxml import etree\n",
    "\n",
    "# Define the path to the annotations directory\n",
    "ann_dir = \"Annotations\"\n",
    "\n",
    "# Define the list of class names\n",
    "class_names = [\"Hydrant\", \"Mail Box & Street Names\", \"Traffic Light\",\"Human Beings\"]\n",
    "\n",
    "# Initialize the list of bounding boxes and labels\n",
    "bboxes_list = []\n",
    "labels_list = []\n",
    "\n",
    "# Loop over the annotated XML files\n",
    "for ann_file in os.listdir(ann_dir):\n",
    "    # Parse the XML file\n",
    "    root = etree.parse(os.path.join(ann_dir, ann_file))\n",
    "\n",
    "    # Extract the image width and height\n",
    "    width = int(root.find(\"size/width\").text)\n",
    "    height = int(root.find(\"size/height\").text)\n",
    "\n",
    "    # Loop over the objects in the XML file\n",
    "    for obj in root.findall(\"object\"):\n",
    "        # Extract the class name\n",
    "        class_name = obj.find(\"name\").text\n",
    "\n",
    "        # Extract the bounding box coordinates\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin = int(float(bbox.find(\"xmin\").text))\n",
    "        ymin = int(float(bbox.find(\"ymin\").text))\n",
    "        xmax = int(float(bbox.find(\"xmax\").text))\n",
    "        ymax = int(float(bbox.find(\"ymax\").text))\n",
    "\n",
    "        # Convert the bounding box coordinates to relative values\n",
    "        xmin = xmin / width\n",
    "        ymin = ymin / height\n",
    "        xmax = xmax / width\n",
    "        ymax = ymax / height\n",
    "\n",
    "        # Append the bounding box and label to the lists\n",
    "        bboxes_list.append([xmin, ymin, xmax, ymax])\n",
    "        labels_list.append(class_names.index(class_name))\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "bboxes = np.array(bboxes_list)\n",
    "labels = np.array(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0297f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input shape for the model\n",
    "input_shape = (None, None, 3)\n",
    "\n",
    "# Define the input layer\n",
    "inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "# Define the backbone network\n",
    "backbone = keras.applications.ResNet50V2(\n",
    "    include_top=False, weights=\"imagenet\", input_tensor=inputs\n",
    ")\n",
    "\n",
    "# Define the Region Proposal Network (RPN)\n",
    "rpn = layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(backbone.output)\n",
    "\n",
    "rpn_class = layers.Conv2D(2, (1, 1), activation=\"sigmoid\")(rpn)\n",
    "\n",
    "rpn_bbox = layers.Conv2D(4, (1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c40608c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "757155cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def faster_rcnn(num_classes):\n",
    "\n",
    "    # Backbone CNN\n",
    "    backbone = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(None, None, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2))\n",
    "    ])\n",
    "\n",
    "    # Region Proposal Network (RPN)\n",
    "    rpn = models.Sequential([\n",
    "        layers.Conv2D(256, (3, 3), padding='same', activation='relu'),\n",
    "        layers.Conv2D(2 * 9, (1, 1), activation='sigmoid'),\n",
    "        layers.Lambda(lambda x: regions.RegionProposalLayer()([x[0], x[1]]))\n",
    "    ])\n",
    "\n",
    "    # Fast R-CNN detector\n",
    "    fast_rcnn_model = models.Sequential([\n",
    "        layers.RoiPooling2D(7, 7),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(4 * num_classes, activation='linear'),\n",
    "        layers.Lambda(lambda x: regions.DetectionsLayer(num_classes)([x[0], x[1]]))\n",
    "    ])\n",
    "\n",
    "    # Inputs\n",
    "    backbone_input = layers.Input(shape=(None, None, 3))\n",
    "    rpn_input = layers.Input(shape=(None, None, 256))\n",
    "    roi_input = layers.Input(shape=(None, 4))\n",
    "\n",
    "    # Intermediate layers\n",
    "    backbone_output = backbone(backbone_input)\n",
    "    rpn_class_output, rpn_bbox_output = rpn(backbone_output)\n",
    "    anchors = layers.AnchorBoxes()(backbone_output)\n",
    "    proposals = layers.ProposalLayer()([rpn_class_output, rpn_bbox_output, anchors])\n",
    "    detections_class_output, detections_bbox_output = fast_rcnn_model([backbone_input, proposals])\n",
    "\n",
    "    # Final model\n",
    "    model = models.Model(inputs=[backbone_input, rpn_input, roi_input], \n",
    "                         outputs=[rpn_class_output, rpn_bbox_output, detections_class_output, detections_bbox_output],\n",
    "                         name='faster_rcnn')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7234cbbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'django.db.models' has no attribute 'Sequential'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mfaster_rcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# assuming we have 10 object classes\u001b[39;00m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the loss function for the RPN\u001b[39;00m\n",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36mfaster_rcnn\u001b[0;34m(num_classes)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfaster_rcnn\u001b[39m(num_classes):\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Backbone CNN\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     backbone \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m([\n\u001b[1;32m      5\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m32\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m3\u001b[39m)),\n\u001b[1;32m      6\u001b[0m         layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m      7\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      8\u001b[0m         layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m      9\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m128\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     10\u001b[0m         layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m     11\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m256\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m         layers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     13\u001b[0m     ])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# Region Proposal Network (RPN)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     rpn \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m     17\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m256\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     18\u001b[0m         layers\u001b[38;5;241m.\u001b[39mConv2D(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m9\u001b[39m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     19\u001b[0m         layers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: regions\u001b[38;5;241m.\u001b[39mRegionProposalLayer()([x[\u001b[38;5;241m0\u001b[39m], x[\u001b[38;5;241m1\u001b[39m]]))\n\u001b[1;32m     20\u001b[0m     ])\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'django.db.models' has no attribute 'Sequential'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = faster_rcnn(10) # assuming we have 10 object classes\n",
    "\n",
    "optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "\n",
    "# Define the loss function for the RPN\n",
    "rpn_cls_loss_fn = regions.rpn_cls_loss(num_classes)\n",
    "rpn_reg_loss_fn = regions.rpn_reg_loss()\n",
    "rpn_loss_fn = lambda y_true, y_pred: rpn_cls_loss_fn(y_true[:,:,:,-18:-9], y_pred[0]) + rpn_reg_loss_fn(y_true[:,:,:,-9:], y_pred[1])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=[rpn_loss_fn, keras.losses.MeanSquaredError()],\n",
    ")\n",
    "\n",
    "\n",
    "# Define the data generator for the model\n",
    "def data_generator(batch_size=32):\n",
    "    while True:\n",
    "        # Randomly select images and their corresponding bounding boxes and labels\n",
    "        indices = np.random.choice(len(images), size=batch_size, replace=False)\n",
    "        batch_images = [images[i] for i in indices]\n",
    "        batch_bboxes = [bboxes[i] for i in indices]\n",
    "        batch_labels = [labels[i] for i in indices]\n",
    "\n",
    "        # Preprocess the images and their corresponding bounding boxes and labels\n",
    "        batch_images, batch_bboxes = preprocess_data(\n",
    "            batch_images, batch_bboxes, input_shape\n",
    "        )\n",
    "\n",
    "        # Generate the anchor boxes for the images\n",
    "        anchor_boxes = generate_anchor_boxes(input_shape)\n",
    "\n",
    "        # Generate the ground truth labels and bounding boxes for the anchor boxes\n",
    "        rpn_labels, rpn_bbox_targets = generate_rpn_targets(\n",
    "            batch_bboxes, anchor_boxes, batch_labels\n",
    "        )\n",
    "\n",
    "        # Yield the data\n",
    "        yield (\n",
    "            batch_images,\n",
    "            (rpn_labels, rpn_bbox_targets),\n",
    "        )\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    data_generator(),\n",
    "    steps_per_epoch=1000,\n",
    "    epochs=10,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7c5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ORC)",
   "language": "python",
   "name": "sys_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
